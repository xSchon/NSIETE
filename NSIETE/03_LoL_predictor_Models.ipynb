{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25839368-5a4f-4dcd-a161-5ce1e0a81849",
   "metadata": {},
   "source": [
    "**School**: Slovak University of technology in Bratislava\\\n",
    "**Faculty**: Faculty of Informatics and Information Technologies\\\n",
    "**Course**: NSIETE\n",
    "\n",
    "**Authors**: Martin Schön and Adam Žák\n",
    "\n",
    "*Seminar*: Wednesday 16:00\\\n",
    "*Seminar teacher*: Mgr. Lukáš Hudec\\\n",
    "*Academic year*: 2022/2023\n",
    "\n",
    "**Dataset**: League of Legends diamond ranked games (10 mins)\\\n",
    "Link: https://www.kaggle.com/datasets/bobbyscience/league-of-legends-diamond-ranked-games-10-min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706c0e2-c41d-4764-b060-cf068ae3cb98",
   "metadata": {},
   "source": [
    "**Notebook description**:\\\n",
    "This notebook serves the purpose of creating models for binary classification of LoL game results, training and evaluating them in PyTorch and Tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca738efa-1905-435a-8dd5-384a3849edfc",
   "metadata": {},
   "source": [
    "### Run notebook 02_LoL_predictor_Data_Preparation before running this one, as it prepares the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c23e31d8-ae7d-4e86-ab03-a80b422dbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ea34fa-070d-48a3-86e6-743dc3235115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging \n",
    "logging.basicConfig(filename=\"training_log.log\",\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c1aad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"   # silence WANDB init as it gets a bit annoying with bigger trainings\n",
    "\n",
    "with open('config.yml', mode=\"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511ae9a3-41a8-4a37-be02-8febf39be1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "DATA_FOLDER = '../data/'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ae76d8-9bf5-453b-9903-9194219749e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_hat, y):\n",
    "    return nn.BCELoss()(y_hat, y)\n",
    "\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81086128-1f17-46e8-b85c-84c709e570d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{DATA_FOLDER}/train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_FOLDER}/test.csv\")\n",
    "validation = pd.read_csv(f\"{DATA_FOLDER}/validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b98cce-5586-4b42-a9fb-c17ab35a3b57",
   "metadata": {},
   "source": [
    "PyTorch needs data prepared in tensors, so preparation and split function is a useful addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70725822-6048-42d8-b3eb-a6816a4eabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pytorch_split(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Split data for prediction and match them to fit PyTorch models\"\"\"\n",
    "    X = df.drop(columns={\"blueWins\"})\n",
    "    y = df.blueWins\n",
    "    \n",
    "    X = torch.tensor(X.values)\n",
    "    y = torch.tensor(y.values)\n",
    "    \n",
    "    # https://stackoverflow.com/a/60440460/12342419\n",
    "    y = y.type(torch.LongTensor)\n",
    "    \n",
    "    X = X.to(torch.float32).to(device)\n",
    "    y = y.to(torch.float32).to(device)\n",
    "    \n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "788d363c-18f4-47b1-874e-03ee9b498ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5144a0531c8c4ae28f6e60a32c9f2ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666912116658447, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/lol-predictor/lol-predictor/runs/1jwqx1tp?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb490d66370>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project=\"lol-predictor\",\n",
    "        config=config['train'],\n",
    "        group='pytorch',\n",
    "        mode='online'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2c2bb66-4c9a-44de-b878-45af8b4651fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = prepare_pytorch_split(train)\n",
    "testX, testy = prepare_pytorch_split(test)\n",
    "valX, valy = prepare_pytorch_split(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd46f8-bc03-493f-a11f-144b9ef2db36",
   "metadata": {},
   "source": [
    "Model definition in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc5bf92-6e7f-4c8b-9306-c3e827c0882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = config['train']['inputNeurons']\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(config[\"preparation\"][\"features_amount\"], nHidden[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[0], nHidden[1]),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[1], nHidden[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[2], nHidden[3]),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[3], 1),\n",
    "    nn.Sigmoid()\n",
    "    # Softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dbb63ea-abe3-4f01-907a-52b9da83fd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): ReLU()\n",
       "  (10): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (11): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ff4e17-a3d1-4932-9407-0d27a6cc27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "130a06ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2683360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99763484-c131-475d-9e4e-ef4114d3f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrain = torch.utils.data.TensorDataset(trainX, trainy)\n",
    "loaderTrain = torch.utils.data.DataLoader(\n",
    "    datasetTrain,\n",
    "    batch_size=config['train']['batchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "datasetTest = torch.utils.data.TensorDataset(testX, testy)\n",
    "loaderTest = torch.utils.data.DataLoader(\n",
    "    datasetTest,\n",
    "    batch_size=config['train']['batchSize'],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328adef-1fe9-409b-bd2d-45312890231e",
   "metadata": {},
   "source": [
    "Main function for model Neural Network training. Currently deprecated, as is used in bigger function later, but the code kept here as a showcase of our training. It validates the epoch on test accuracy and loss and logs into wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a42c8922-f05b-46f9-8240-1f0dd506ee9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = config['train']['epochs']\n",
    "\n",
    "# disabled for now \n",
    "for n in range(0):\n",
    "    model.train()\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "\n",
    "    for (x,y) in tqdm(loaderTrain):\n",
    "        y_pred = model(x)\n",
    "        loss = compute_loss(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum = loss_sum + loss.cpu().detach().item()\n",
    "        accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "    train_accuracy = accuracy_sum / len(loaderTrain)\n",
    "    train_loss = loss_sum / len(loaderTrain)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accuracy_sum = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        for (x,y) in tqdm(loaderTest):\n",
    "            y_pred = model(x)\n",
    "            val_loss = compute_loss(y_pred, y)\n",
    "\n",
    "            loss_sum = loss_sum + val_loss.cpu().detach().item()\n",
    "            accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "    test_accuracy = accuracy_sum / len(loaderTest)\n",
    "    train_loss = loss_sum / len(loaderTest)\n",
    "\n",
    "\n",
    "    print(f'Epocha: {n}')\n",
    "    print(f'Test accuracy: {test_accuracy} - Test Loss: {val_loss} |<->| Train Accuracy: {train_accuracy} - Train Loss: {train_loss}')\n",
    "    wandb.log({'epoch': n, 'test_accuracy': test_accuracy, 'loss_val': val_loss, 'train_accuracy': train_accuracy, 'loss_train': train_loss})\n",
    "#print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80a67b2b-86b5-456c-8518-9117127bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4f653-d201-41ab-9211-7f3a01391124",
   "metadata": {},
   "source": [
    "## Pytorch hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fcda4-7a11-4278-b581-d3a086f59409",
   "metadata": {},
   "source": [
    "This part contains duplicated code from above, used for hyperparameter tuning of a model.\\\n",
    "Function pytorch_training takes model to train and config to try and outputs accuracy and wandb graphs.\\\n",
    "We used this function for training with different parameters and architectures to find the best outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc3c81b4-c153-415d-b414-152694dab17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_training(config : dict, model):\n",
    "    \"\"\"Hyperparameter tuning of pytorch models.\"\"\"\n",
    "    run_name = f\"{config['model_name']}-lr{config['train']['lr']}-ep{config['train']['epochs']}-bs{config['train']['batchSize']}\"\n",
    "    wandb.init(\n",
    "            project=\"lol-predictor\",\n",
    "            config=config['train'],\n",
    "            group='pytorch',\n",
    "            mode='online',\n",
    "            name=run_name\n",
    "        )\n",
    " \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])\n",
    "    \n",
    "    trainX, trainy = prepare_pytorch_split(train)\n",
    "    testX, testy = prepare_pytorch_split(test)\n",
    "    valX, valy = prepare_pytorch_split(validation)\n",
    "    \n",
    "    model.to(device)\n",
    "    datasetTrain = torch.utils.data.TensorDataset(trainX, trainy)\n",
    "    loaderTrain = torch.utils.data.DataLoader(\n",
    "        datasetTrain,\n",
    "        batch_size=config['train']['batchSize'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    datasetTest = torch.utils.data.TensorDataset(testX, testy)\n",
    "    loaderTest = torch.utils.data.DataLoader(\n",
    "        datasetTest,\n",
    "        batch_size=config['train']['batchSize'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for n in range(config['train']['epochs']):\n",
    "        model.train()\n",
    "        accuracy_sum = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        for (x,y) in tqdm(loaderTrain):\n",
    "            y_pred = model(x)\n",
    "            loss = compute_loss(y_pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum = loss_sum + loss.cpu().detach().item()\n",
    "            accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "        train_accuracy = accuracy_sum / len(loaderTrain)\n",
    "        train_loss = loss_sum / len(loaderTrain)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            accuracy_sum = 0\n",
    "            loss_sum = 0\n",
    "            recall_sum = 0\n",
    "            precision_sum = 0\n",
    "\n",
    "            for (x,y) in tqdm(loaderTest):                \n",
    "                y_pred = model(x)\n",
    "                val_loss = compute_loss(y_pred, y)\n",
    "\n",
    "                loss_sum = loss_sum + val_loss.cpu().detach().item()\n",
    "                accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "                cpu_y = torch.round(y).cpu().detach().numpy()\n",
    "                cpu_pred_y = torch.round(y_pred).cpu().detach().numpy()\n",
    "\n",
    "                recall_sum += recall_score(y_true=cpu_y, y_pred=cpu_pred_y)\n",
    "                precision_sum += precision_score(cpu_y, y_pred=cpu_pred_y)\n",
    "\n",
    "        test_accuracy = accuracy_sum / len(loaderTest)\n",
    "        train_loss = loss_sum / len(loaderTest)\n",
    "        recall = recall_sum / len(loaderTest)\n",
    "        precision = precision_sum / len(loaderTest)\n",
    "\n",
    "        wandb.log({'epoch': n,\n",
    "                   'test_accuracy': test_accuracy, \n",
    "                   'test_loss': val_loss, \n",
    "                   'train_accuracy': train_accuracy, \n",
    "                   'train_loss': train_loss,\n",
    "                   'test_precision': precision,\n",
    "                   'test_recall': recall\n",
    "                   })\n",
    "        if best_accuracy < test_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "    \n",
    "    np_valy = valy.cpu().detach().numpy()\n",
    "    np_predy = torch.round(model(valX)).cpu().detach().numpy()\n",
    "    logging.info(f\"Validation metrics: \\n    Accuracy: {round(accuracy_score(np_valy, np_predy), 4)}\\n\\\n",
    "    Recall: {round(recall_score(np_valy, np_predy), 4)}\\n    Precision: {round(precision_score(np_valy, np_predy), 4)}.\")\n",
    "    return (best_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3c32e-7994-4aeb-b5ff-76e4356d3773",
   "metadata": {},
   "source": [
    "Different model architectures we've tried in hyperparameter tuning. There is little data for this dataset, so we had a bit of a overfitting problem. Smaller models seem to work good, because of the lack of data, so we tested more variants. If user wants to try more models, they can just add more architectures to get_model function and add them to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99eedef3-5a6d-4a39-bcb8-6fcb7bef74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name : str):\n",
    "    \"\"\"Get empty model to train based on your choice\"\"\"\n",
    "    nHidden = config['train']['inputNeurons']\n",
    "    \n",
    "    if name == 'baseline':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], nHidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[0], nHidden[1]),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[1], nHidden[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[2], nHidden[3]),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[3], 1),\n",
    "            nn.Sigmoid()\n",
    "            # Softmax\n",
    "        )\n",
    "    \n",
    "    if name == 'small':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    if name == 'nano':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    if name == 'LeakyNano':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a32a02ef-5794-40e5-9087-9df800e3f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_search = {'lr': [0.01, 0.005, 0.001, 0.0005],\n",
    "              'epochs': [25, 50, 100, 200],\n",
    "              'batchSize': [256, 512, 99999],\n",
    "              'model_names': ['baseline', 'small', 'nano', 'LeakyNano']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54b28b73-bc47-401d-8fcd-b3278b6e0222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhyp_search = {'lr': [0.01],\\n              'epochs': [200],\\n              'batchSize': [6385],\\n              'model_names': ['baseline']\\n            }\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyp_search = {'lr': [0.01],\n",
    "              'epochs': [200],\n",
    "              'batchSize': [6385],\n",
    "              'model_names': ['baseline']\n",
    "            }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bdfb4-f4ff-48fa-a6c7-baee8a57d872",
   "metadata": {},
   "source": [
    "Test different parameters and model architectures and evaluate on validation set with accuracy, recall and precission score as result.\\\n",
    "Higher number in all department is better, with scale being 0-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40555a8-ed43-476c-b3ea-d27d9456d58a",
   "metadata": {},
   "source": [
    "Run hyperparameter gridsearch on different models and chosen features defined in hyp_search. Log them into wandb, log their validation values and try different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbd5ae65-05e9-4619-bbe4-17f92a0f878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# silence everything, but prints from this cell\n",
    "# 53m 28sec\n",
    "\n",
    "test_conf = {'train' : {}}\n",
    "\n",
    "for testing_model in hyp_search[\"model_names\"]:\n",
    "    test_conf['model_name'] = testing_model\n",
    "    for learn_rate in hyp_search['lr']:\n",
    "        test_conf['train']['lr'] = learn_rate\n",
    "\n",
    "        for epochs in hyp_search['epochs']:\n",
    "            test_conf['train']['epochs'] = epochs\n",
    "\n",
    "            for batchSize in hyp_search['batchSize']:\n",
    "                test_conf['train']['batchSize'] = batchSize\n",
    "                logging.info(f\"Training - {testing_model}: {test_conf['train']}\")\n",
    "                best_acc, final_acc = pytorch_training(test_conf, get_model(testing_model))\n",
    "                logging.info(f\"Results - Best accuracy on test: {round(best_acc, 4)}, finished on {round(final_acc, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e45216-6aca-460c-8ecc-b43daf9b1843",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a10d1a6e-2753-479d-bcdb-c0fed4dbfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3467c313-5e1c-4dc1-a9a5-dedce4df9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2a6f8e547c43c5aa7a1c4403b66d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669612416687112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/lol-predictor/lol-predictor/runs/3pr02sfz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fea2c112160>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "        project=\"lol-predictor\",\n",
    "        config=config['train'],\n",
    "        group='tensorflow',\n",
    "        mode='online'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71c12c75-af4b-4927-b112-0e685d0e821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = train.drop(columns={\"blueWins\"})\n",
    "trainy = train['blueWins']\n",
    "testX = test.drop(columns={\"blueWins\"})\n",
    "testy = test['blueWins']\n",
    "valX = validation.drop(columns={\"blueWins\"})\n",
    "valy = validation[\"blueWins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8dbc0241-43e5-40b8-999c-86a117146ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = config['train']['inputNeurons']\n",
    "\n",
    "tf_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=20, activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[0], activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[1], activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[2], activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[3], activation='relu'),  #activation='softmax'\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tf_model.compile(optimizer='adam', \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a4c5a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = config['train']['inputNeurons']\n",
    "\n",
    "tf_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=config[\"preparation\"][\"features_amount\"], activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tf_model.compile(optimizer=keras.optimizers.Adam(lr=config['train']['lr']),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f4d86b5c-361b-4bad-ba98-07dd0b6fff05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 8ms/step - loss: 0.6241 - accuracy: 0.6717 - val_loss: 0.5905 - val_accuracy: 0.6930\n",
      "Epoch 2/50\n",
      " 1/50 [..............................] - ETA: 0s - loss: 0.6226 - accuracy: 0.6484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5769 - accuracy: 0.7009 - val_loss: 0.5616 - val_accuracy: 0.7105\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7088 - val_loss: 0.5521 - val_accuracy: 0.7206\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5551 - accuracy: 0.7096 - val_loss: 0.5459 - val_accuracy: 0.7166\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5534 - accuracy: 0.7135 - val_loss: 0.5461 - val_accuracy: 0.7227\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5507 - accuracy: 0.7153 - val_loss: 0.5455 - val_accuracy: 0.7240\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5504 - accuracy: 0.7154 - val_loss: 0.5475 - val_accuracy: 0.7247\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5498 - accuracy: 0.7145 - val_loss: 0.5448 - val_accuracy: 0.7186\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5509 - accuracy: 0.7143 - val_loss: 0.5462 - val_accuracy: 0.7227\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5498 - accuracy: 0.7162 - val_loss: 0.5474 - val_accuracy: 0.7233\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5494 - accuracy: 0.7153 - val_loss: 0.5441 - val_accuracy: 0.7240\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5494 - accuracy: 0.7123 - val_loss: 0.5451 - val_accuracy: 0.7247\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5484 - accuracy: 0.7151 - val_loss: 0.5450 - val_accuracy: 0.7220\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5483 - accuracy: 0.7165 - val_loss: 0.5445 - val_accuracy: 0.7227\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5475 - accuracy: 0.7145 - val_loss: 0.5532 - val_accuracy: 0.7200\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.7137 - val_loss: 0.5449 - val_accuracy: 0.7173\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5497 - accuracy: 0.7146 - val_loss: 0.5444 - val_accuracy: 0.7220\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5479 - accuracy: 0.7148 - val_loss: 0.5475 - val_accuracy: 0.7247\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5490 - accuracy: 0.7134 - val_loss: 0.5446 - val_accuracy: 0.7274\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5484 - accuracy: 0.7148 - val_loss: 0.5461 - val_accuracy: 0.7247\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5475 - accuracy: 0.7156 - val_loss: 0.5443 - val_accuracy: 0.7247\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7117 - val_loss: 0.5444 - val_accuracy: 0.7220\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5478 - accuracy: 0.7128 - val_loss: 0.5467 - val_accuracy: 0.7247\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5472 - accuracy: 0.7132 - val_loss: 0.5448 - val_accuracy: 0.7281\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.7168 - val_loss: 0.5454 - val_accuracy: 0.7287\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5473 - accuracy: 0.7146 - val_loss: 0.5445 - val_accuracy: 0.7254\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5476 - accuracy: 0.7175 - val_loss: 0.5452 - val_accuracy: 0.7240\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5465 - accuracy: 0.7165 - val_loss: 0.5453 - val_accuracy: 0.7281\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.7171 - val_loss: 0.5453 - val_accuracy: 0.7281\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5475 - accuracy: 0.7151 - val_loss: 0.5435 - val_accuracy: 0.7267\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5467 - accuracy: 0.7173 - val_loss: 0.5468 - val_accuracy: 0.7233\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5466 - accuracy: 0.7165 - val_loss: 0.5461 - val_accuracy: 0.7260\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.7143 - val_loss: 0.5452 - val_accuracy: 0.7240\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.7167 - val_loss: 0.5455 - val_accuracy: 0.7233\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.7131 - val_loss: 0.5457 - val_accuracy: 0.7287\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.7145 - val_loss: 0.5445 - val_accuracy: 0.7240\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5474 - accuracy: 0.7187 - val_loss: 0.5471 - val_accuracy: 0.7220\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.7145 - val_loss: 0.5442 - val_accuracy: 0.7260\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.7161 - val_loss: 0.5448 - val_accuracy: 0.7281\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5466 - accuracy: 0.7182 - val_loss: 0.5444 - val_accuracy: 0.7260\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5461 - accuracy: 0.7134 - val_loss: 0.5465 - val_accuracy: 0.7274\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5469 - accuracy: 0.7157 - val_loss: 0.5466 - val_accuracy: 0.7260\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5465 - accuracy: 0.7168 - val_loss: 0.5478 - val_accuracy: 0.7247\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5464 - accuracy: 0.7154 - val_loss: 0.5484 - val_accuracy: 0.7254\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5450 - accuracy: 0.7182 - val_loss: 0.5452 - val_accuracy: 0.7240\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5456 - accuracy: 0.7170 - val_loss: 0.5446 - val_accuracy: 0.7233\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5455 - accuracy: 0.7156 - val_loss: 0.5465 - val_accuracy: 0.7247\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5454 - accuracy: 0.7186 - val_loss: 0.5448 - val_accuracy: 0.7233\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5448 - accuracy: 0.7186 - val_loss: 0.5489 - val_accuracy: 0.7213\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5451 - accuracy: 0.7154 - val_loss: 0.5463 - val_accuracy: 0.7267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea0c4ac370>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, batch_size=32, callbacks=[WandbMetricsLogger(log_freq=32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8fdc82cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7354926"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = tf_model.predict(valX)\n",
    "\n",
    "accuracy_calc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "accuracy_calc.update_state(valy, y_hat)\n",
    "accuracy_calc.result().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
