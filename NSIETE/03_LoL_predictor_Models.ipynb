{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25839368-5a4f-4dcd-a161-5ce1e0a81849",
   "metadata": {},
   "source": [
    "**School**: Slovak University of technology in Bratislava\\\n",
    "**Faculty**: Faculty of Informatics and Information Technologies\\\n",
    "**Course**: NSIETE\n",
    "\n",
    "**Authors**: Martin Schön and Adam Žák\n",
    "\n",
    "*Seminar*: Wednesday 16:00\\\n",
    "*Seminar teacher*: Mgr. Lukáš Hudec\\\n",
    "*Academic year*: 2022/2023\n",
    "\n",
    "**Dataset**: League of Legends diamond ranked games (10 mins)\\\n",
    "Link: https://www.kaggle.com/datasets/bobbyscience/league-of-legends-diamond-ranked-games-10-min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706c0e2-c41d-4764-b060-cf068ae3cb98",
   "metadata": {},
   "source": [
    "**Notebook description**:\\\n",
    "This notebook serves the purpose of creating models for binary classification of LoL game results, training and evaluating them in PyTorch and Tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca738efa-1905-435a-8dd5-384a3849edfc",
   "metadata": {},
   "source": [
    "### Run notebook 02_LoL_predictor_Data_Preparation before running this one, as it prepares the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c23e31d8-ae7d-4e86-ab03-a80b422dbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ea34fa-070d-48a3-86e6-743dc3235115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging \n",
    "logging.basicConfig(filename=\"training_log.log\",\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c1aad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"   # silence WANDB init as it gets a bit annoying with bigger trainings\n",
    "\n",
    "with open('config.yml', mode=\"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511ae9a3-41a8-4a37-be02-8febf39be1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "DATA_FOLDER = '../data/'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ae76d8-9bf5-453b-9903-9194219749e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_hat, y):\n",
    "    return nn.BCELoss()(y_hat, y)\n",
    "\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81086128-1f17-46e8-b85c-84c709e570d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{DATA_FOLDER}/train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_FOLDER}/test.csv\")\n",
    "validation = pd.read_csv(f\"{DATA_FOLDER}/validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b98cce-5586-4b42-a9fb-c17ab35a3b57",
   "metadata": {},
   "source": [
    "PyTorch needs data prepared in tensors, so preparation and split function is a useful addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70725822-6048-42d8-b3eb-a6816a4eabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pytorch_split(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Split data for prediction and match them to fit PyTorch models\"\"\"\n",
    "    X = df.drop(columns={\"blueWins\"})\n",
    "    y = df.blueWins\n",
    "    \n",
    "    X = torch.tensor(X.values)\n",
    "    y = torch.tensor(y.values)\n",
    "    \n",
    "    # https://stackoverflow.com/a/60440460/12342419\n",
    "    y = y.type(torch.LongTensor)\n",
    "    \n",
    "    X = X.to(torch.float32).to(device)\n",
    "    y = y.to(torch.float32).to(device)\n",
    "    \n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "788d363c-18f4-47b1-874e-03ee9b498ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5144a0531c8c4ae28f6e60a32c9f2ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666912116658447, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/lol-predictor/lol-predictor/runs/1jwqx1tp?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb490d66370>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project=\"lol-predictor\",\n",
    "        config=config['train'],\n",
    "        group='pytorch',\n",
    "        mode='online'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2c2bb66-4c9a-44de-b878-45af8b4651fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = prepare_pytorch_split(train)\n",
    "testX, testy = prepare_pytorch_split(test)\n",
    "valX, valy = prepare_pytorch_split(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd46f8-bc03-493f-a11f-144b9ef2db36",
   "metadata": {},
   "source": [
    "Model definition in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc5bf92-6e7f-4c8b-9306-c3e827c0882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = config['train']['inputNeurons']\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(config[\"preparation\"][\"features_amount\"], nHidden[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[0], nHidden[1]),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[1], nHidden[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[2], nHidden[3]),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nHidden[3], 1),\n",
    "    nn.Sigmoid()\n",
    "    # Softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dbb63ea-abe3-4f01-907a-52b9da83fd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): ReLU()\n",
       "  (10): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (11): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ff4e17-a3d1-4932-9407-0d27a6cc27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "130a06ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2683360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99763484-c131-475d-9e4e-ef4114d3f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrain = torch.utils.data.TensorDataset(trainX, trainy)\n",
    "loaderTrain = torch.utils.data.DataLoader(\n",
    "    datasetTrain,\n",
    "    batch_size=config['train']['batchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "datasetTest = torch.utils.data.TensorDataset(testX, testy)\n",
    "loaderTest = torch.utils.data.DataLoader(\n",
    "    datasetTest,\n",
    "    batch_size=config['train']['batchSize'],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328adef-1fe9-409b-bd2d-45312890231e",
   "metadata": {},
   "source": [
    "Main function for model Neural Network training. Currently deprecated, as is used in bigger function later, but the code kept here as a showcase of our training. It validates the epoch on test accuracy and loss and logs into wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a42c8922-f05b-46f9-8240-1f0dd506ee9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = config['train']['epochs']\n",
    "\n",
    "# disabled for now \n",
    "for n in range(0):\n",
    "    model.train()\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "\n",
    "    for (x,y) in tqdm(loaderTrain):\n",
    "        y_pred = model(x)\n",
    "        loss = compute_loss(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum = loss_sum + loss.cpu().detach().item()\n",
    "        accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "    train_accuracy = accuracy_sum / len(loaderTrain)\n",
    "    train_loss = loss_sum / len(loaderTrain)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accuracy_sum = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        for (x,y) in tqdm(loaderTest):\n",
    "            y_pred = model(x)\n",
    "            val_loss = compute_loss(y_pred, y)\n",
    "\n",
    "            loss_sum = loss_sum + val_loss.cpu().detach().item()\n",
    "            accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "    test_accuracy = accuracy_sum / len(loaderTest)\n",
    "    train_loss = loss_sum / len(loaderTest)\n",
    "\n",
    "\n",
    "    print(f'Epocha: {n}')\n",
    "    print(f'Test accuracy: {test_accuracy} - Test Loss: {val_loss} |<->| Train Accuracy: {train_accuracy} - Train Loss: {train_loss}')\n",
    "    wandb.log({'epoch': n, 'test_accuracy': test_accuracy, 'loss_val': val_loss, 'train_accuracy': train_accuracy, 'loss_train': train_loss})\n",
    "#print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80a67b2b-86b5-456c-8518-9117127bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4f653-d201-41ab-9211-7f3a01391124",
   "metadata": {},
   "source": [
    "## Pytorch hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fcda4-7a11-4278-b581-d3a086f59409",
   "metadata": {},
   "source": [
    "This part contains duplicated code from above, used for hyperparameter tuning of a model.\\\n",
    "Function pytorch_training takes model to train and config to try and outputs accuracy and wandb graphs.\\\n",
    "We used this function for training with different parameters and architectures to find the best outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc3c81b4-c153-415d-b414-152694dab17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_training(config : dict, model):\n",
    "    \"\"\"Hyperparameter tuning of pytorch models.\"\"\"\n",
    "    run_name = f\"{config['model_name']}-lr{config['train']['lr']}-ep{config['train']['epochs']}-bs{config['train']['batchSize']}\"\n",
    "    wandb.init(\n",
    "            project=\"lol-predictor\",\n",
    "            config=config['train'],\n",
    "            group='pytorch',\n",
    "            mode='online',\n",
    "            name=run_name\n",
    "        )\n",
    " \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])\n",
    "    \n",
    "    trainX, trainy = prepare_pytorch_split(train)\n",
    "    testX, testy = prepare_pytorch_split(test)\n",
    "    valX, valy = prepare_pytorch_split(validation)\n",
    "    \n",
    "    model.to(device)\n",
    "    datasetTrain = torch.utils.data.TensorDataset(trainX, trainy)\n",
    "    loaderTrain = torch.utils.data.DataLoader(\n",
    "        datasetTrain,\n",
    "        batch_size=config['train']['batchSize'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    datasetTest = torch.utils.data.TensorDataset(testX, testy)\n",
    "    loaderTest = torch.utils.data.DataLoader(\n",
    "        datasetTest,\n",
    "        batch_size=config['train']['batchSize'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for n in range(config['train']['epochs']):\n",
    "        model.train()\n",
    "        accuracy_sum = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        for (x,y) in tqdm(loaderTrain):\n",
    "            y_pred = model(x)\n",
    "            loss = compute_loss(y_pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum = loss_sum + loss.cpu().detach().item()\n",
    "            accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "        train_accuracy = accuracy_sum / len(loaderTrain)\n",
    "        train_loss = loss_sum / len(loaderTrain)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            accuracy_sum = 0\n",
    "            loss_sum = 0\n",
    "            recall_sum = 0\n",
    "            precision_sum = 0\n",
    "\n",
    "            for (x,y) in tqdm(loaderTest):                \n",
    "                y_pred = model(x)\n",
    "                val_loss = compute_loss(y_pred, y)\n",
    "\n",
    "                loss_sum = loss_sum + val_loss.cpu().detach().item()\n",
    "                accuracy_sum = accuracy_sum + accuracy_fn(y_true=y, y_pred=torch.round(y_pred))\n",
    "\n",
    "                cpu_y = torch.round(y).cpu().detach().numpy()\n",
    "                cpu_pred_y = torch.round(y_pred).cpu().detach().numpy()\n",
    "\n",
    "                recall_sum += recall_score(y_true=cpu_y, y_pred=cpu_pred_y)\n",
    "                precision_sum += precision_score(cpu_y, y_pred=cpu_pred_y)\n",
    "\n",
    "        test_accuracy = accuracy_sum / len(loaderTest)\n",
    "        train_loss = loss_sum / len(loaderTest)\n",
    "        recall = recall_sum / len(loaderTest)\n",
    "        precision = precision_sum / len(loaderTest)\n",
    "\n",
    "        wandb.log({'epoch': n,\n",
    "                   'test_accuracy': test_accuracy, \n",
    "                   'test_loss': val_loss, \n",
    "                   'train_accuracy': train_accuracy, \n",
    "                   'train_loss': train_loss,\n",
    "                   'test_precision': precision,\n",
    "                   'test_recall': recall\n",
    "                   })\n",
    "        if best_accuracy < test_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "    \n",
    "    np_valy = valy.cpu().detach().numpy()\n",
    "    np_predy = torch.round(model(valX)).cpu().detach().numpy()\n",
    "    logging.info(f\"Validation metrics: \\n    Accuracy: {round(accuracy_score(np_valy, np_predy), 4)}\\n\\\n",
    "    Recall: {round(recall_score(np_valy, np_predy), 4)}\\n    Precision: {round(precision_score(np_valy, np_predy), 4)}.\")\n",
    "    return (best_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3c32e-7994-4aeb-b5ff-76e4356d3773",
   "metadata": {},
   "source": [
    "Different model architectures we've tried in hyperparameter tuning. There is little data for this dataset, so we had a bit of a overfitting problem. Smaller models seem to work good, because of the lack of data, so we tested more variants. If user wants to try more models, they can just add more architectures to get_model function and add them to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99eedef3-5a6d-4a39-bcb8-6fcb7bef74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name : str):\n",
    "    \"\"\"Get empty model to train based on your choice\"\"\"\n",
    "    nHidden = config['train']['inputNeurons']\n",
    "    \n",
    "    if name == 'baseline':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], nHidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[0], nHidden[1]),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[1], nHidden[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[2], nHidden[3]),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nHidden[3], 1),\n",
    "            nn.Sigmoid()\n",
    "            # Softmax\n",
    "        )\n",
    "    \n",
    "    if name == 'small':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    if name == 'nano':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    if name == 'LeakyNano':\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(config[\"preparation\"][\"features_amount\"], 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a32a02ef-5794-40e5-9087-9df800e3f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_search = {'lr': [0.01, 0.005, 0.001, 0.0005],\n",
    "              'epochs': [25, 50, 100, 200],\n",
    "              'batchSize': [256, 512, 99999],\n",
    "              'model_names': ['baseline', 'small', 'nano', 'LeakyNano']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54b28b73-bc47-401d-8fcd-b3278b6e0222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhyp_search = {'lr': [0.01],\\n              'epochs': [200],\\n              'batchSize': [6385],\\n              'model_names': ['baseline']\\n            }\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyp_search = {'lr': [0.01],\n",
    "              'epochs': [200],\n",
    "              'batchSize': [6385],\n",
    "              'model_names': ['baseline']\n",
    "            }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bdfb4-f4ff-48fa-a6c7-baee8a57d872",
   "metadata": {},
   "source": [
    "Test different parameters and model architectures and evaluate on validation set with accuracy, recall and precission score as result.\\\n",
    "Higher number in all department is better, with scale being 0-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40555a8-ed43-476c-b3ea-d27d9456d58a",
   "metadata": {},
   "source": [
    "Run hyperparameter gridsearch on different models and chosen features defined in hyp_search. Log them into wandb, log their validation values and try different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbd5ae65-05e9-4619-bbe4-17f92a0f878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# silence everything, but prints from this cell\n",
    "# 53m 28sec\n",
    "\n",
    "test_conf = {'train' : {}}\n",
    "\n",
    "for testing_model in hyp_search[\"model_names\"]:\n",
    "    test_conf['model_name'] = testing_model\n",
    "    for learn_rate in hyp_search['lr']:\n",
    "        test_conf['train']['lr'] = learn_rate\n",
    "\n",
    "        for epochs in hyp_search['epochs']:\n",
    "            test_conf['train']['epochs'] = epochs\n",
    "\n",
    "            for batchSize in hyp_search['batchSize']:\n",
    "                test_conf['train']['batchSize'] = batchSize\n",
    "                logging.info(f\"Training - {testing_model}: {test_conf['train']}\")\n",
    "                best_acc, final_acc = pytorch_training(test_conf, get_model(testing_model))\n",
    "                logging.info(f\"Results - Best accuracy on test: {round(best_acc, 4)}, finished on {round(final_acc, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e45216-6aca-460c-8ecc-b43daf9b1843",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a10d1a6e-2753-479d-bcdb-c0fed4dbfe0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_126822/47211615.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbMetricsLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from wandb.integration.keras import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mWandbCallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mWandbEvalCallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/integration/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbEvalCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWandbMetricsLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWandbModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbCallback\u001b[0m  \u001b[0;31m# todo: legacy callback to be deprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/integration/keras/callbacks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"WandbMetricsLogger\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WandbModelCheckpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WandbEvalCallback\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbMetricsLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtables_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbEvalCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/integration/keras/callbacks/metrics_logger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3467c313-5e1c-4dc1-a9a5-dedce4df9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035c20781292455897715d901262d3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668312216643243, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lol-predictor/lol-predictor/runs/rq1n6wuc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f33154e8f70>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "        project=\"lol-predictor\",\n",
    "        config=config['train'],\n",
    "        group='tensorflow',\n",
    "        mode='online'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dbc0241-43e5-40b8-999c-86a117146ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = config['train']['inputNeurons']\n",
    "\n",
    "tf_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=20, activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[0], activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[1], activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[2], activation='relu'),\n",
    "    keras.layers.Dense(units=nHidden[3], activation='relu'),  #activation='softmax'\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tf_model.compile(optimizer='adam', \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71c12c75-af4b-4927-b112-0e685d0e821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = train.drop(columns={\"blueWins\"})\n",
    "trainy = train['blueWins']\n",
    "testX = test.drop(columns={\"blueWins\"})\n",
    "testy = test['blueWins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4d86b5c-361b-4bad-ba98-07dd0b6fff05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xschon/.local/lib/python3.10/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 2ms/step - loss: 0.6062 - accuracy: 0.6684 - val_loss: 0.5506 - val_accuracy: 0.7267\n",
      "Epoch 2/50\n",
      " 36/200 [====>.........................] - ETA: 0s - loss: 0.5420 - accuracy: 0.7274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xschon/.local/lib/python3.10/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7142 - val_loss: 0.5581 - val_accuracy: 0.7173\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7156 - val_loss: 0.5490 - val_accuracy: 0.7301\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7145 - val_loss: 0.5460 - val_accuracy: 0.7321\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5484 - accuracy: 0.7206 - val_loss: 0.5514 - val_accuracy: 0.7294\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7211 - val_loss: 0.5771 - val_accuracy: 0.7031\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7181 - val_loss: 0.5450 - val_accuracy: 0.7308\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7206 - val_loss: 0.5495 - val_accuracy: 0.7247\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5450 - accuracy: 0.7261 - val_loss: 0.5516 - val_accuracy: 0.7247\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7208 - val_loss: 0.5542 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5452 - accuracy: 0.7230 - val_loss: 0.5497 - val_accuracy: 0.7233\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5458 - accuracy: 0.7234 - val_loss: 0.5437 - val_accuracy: 0.7233\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.7208 - val_loss: 0.5479 - val_accuracy: 0.7227\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7234 - val_loss: 0.5490 - val_accuracy: 0.7294\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7244 - val_loss: 0.5477 - val_accuracy: 0.7267\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7225 - val_loss: 0.5455 - val_accuracy: 0.7308\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7239 - val_loss: 0.5452 - val_accuracy: 0.7220\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.7219 - val_loss: 0.5565 - val_accuracy: 0.7132\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5409 - accuracy: 0.7247 - val_loss: 0.5471 - val_accuracy: 0.7220\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.7242 - val_loss: 0.5507 - val_accuracy: 0.7240\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7270 - val_loss: 0.5451 - val_accuracy: 0.7308\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7274 - val_loss: 0.5529 - val_accuracy: 0.7159\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7223 - val_loss: 0.5476 - val_accuracy: 0.7294\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7270 - val_loss: 0.5488 - val_accuracy: 0.7314\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5378 - accuracy: 0.7220 - val_loss: 0.5568 - val_accuracy: 0.7227\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5380 - accuracy: 0.7258 - val_loss: 0.5489 - val_accuracy: 0.7227\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7278 - val_loss: 0.5484 - val_accuracy: 0.7301\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5377 - accuracy: 0.7283 - val_loss: 0.5488 - val_accuracy: 0.7233\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5360 - accuracy: 0.7266 - val_loss: 0.5516 - val_accuracy: 0.7233\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5354 - accuracy: 0.7266 - val_loss: 0.5508 - val_accuracy: 0.7254\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7314 - val_loss: 0.5543 - val_accuracy: 0.7159\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7288 - val_loss: 0.5595 - val_accuracy: 0.7220\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5323 - accuracy: 0.7308 - val_loss: 0.5530 - val_accuracy: 0.7240\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7294 - val_loss: 0.5575 - val_accuracy: 0.7240\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5319 - accuracy: 0.7288 - val_loss: 0.5508 - val_accuracy: 0.7254\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7349 - val_loss: 0.5526 - val_accuracy: 0.7274\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7338 - val_loss: 0.5535 - val_accuracy: 0.7274\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7319 - val_loss: 0.5573 - val_accuracy: 0.7287\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5293 - accuracy: 0.7297 - val_loss: 0.5649 - val_accuracy: 0.7260\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5277 - accuracy: 0.7336 - val_loss: 0.5560 - val_accuracy: 0.7213\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7357 - val_loss: 0.5575 - val_accuracy: 0.7267\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.7330 - val_loss: 0.5626 - val_accuracy: 0.7166\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7341 - val_loss: 0.5550 - val_accuracy: 0.7254\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7352 - val_loss: 0.5609 - val_accuracy: 0.7166\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7408 - val_loss: 0.5810 - val_accuracy: 0.7227\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7405 - val_loss: 0.5630 - val_accuracy: 0.7159\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.7371 - val_loss: 0.5674 - val_accuracy: 0.7213\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7386 - val_loss: 0.5602 - val_accuracy: 0.7227\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.7344 - val_loss: 0.5649 - val_accuracy: 0.7173\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7413 - val_loss: 0.5596 - val_accuracy: 0.7206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40d899ed70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, batch_size=32, callbacks=[WandbMetricsLogger(log_freq=32)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
